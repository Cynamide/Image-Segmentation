{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\r\n",
    "import numpy as np\r\n",
    "import skimage.io as io\r\n",
    "import cv2\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
    "\r\n",
    "### For visualizing the outputs ###\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.gridspec as gridspec\r\n",
    "import pathlib\r\n",
    "import PIL\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important variables to be changed according to your preferences (or left alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\r\n",
    "SHUFFLE_BUFFER_SIZE = 1000\r\n",
    "VAL_SIZE = 800\r\n",
    "OUTPUT_CHANNELS = 3\r\n",
    "EPOCHS = 10\r\n",
    "VAL_SPLITS=2\r\n",
    "VALIDATION_STEPS = VAL_SIZE//BATCH_SIZE//VAL_SPLITS\r\n",
    "image_size = (224,224)\r\n",
    "folder = './COCOdataset2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a Tensorflow Dataset with the filenames of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('{}/images/train_img'.format(folder))\r\n",
    "filenames = list(data_dir.glob('*.jpg'))\r\n",
    "print(\"image count: \", len(filenames))\r\n",
    "\r\n",
    "fnames=[]\r\n",
    "for fname in filenames:\r\n",
    "  fnames.append(str(fname))\r\n",
    "\r\n",
    "\r\n",
    "data_dir = pathlib.Path('{}/images/train_mask'.format(folder))\r\n",
    "filenames = list(data_dir.glob('*.jpg'))\r\n",
    "print(\"image count: \", len(filenames))\r\n",
    "\r\n",
    "fnames_mask=[]\r\n",
    "for fname in filenames:\r\n",
    "  fnames_mask.append(str(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = len(filenames) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((fnames, fnames_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the training dataset into validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = train_dataset.skip(VAL_SIZE)\r\n",
    "validation_dataset = train_dataset.take(VAL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating functions to read and convert filenames into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img):\r\n",
    "  print(\"process_img acivated...\")\r\n",
    "  #color images\r\n",
    "  img = tf.image.decode_jpeg(img) \r\n",
    "  #convert unit8 tensor to floats in the [0,1]range\r\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32) \r\n",
    "  return img\r\n",
    "  \r\n",
    "def combine_train_images(file_path,file_path_mask):\r\n",
    "  print(\"combine_images_labels acivated...\")\r\n",
    "  img = tf.io.read_file(file_path)\r\n",
    "  mask = tf.io.read_file(file_path_mask)\r\n",
    "  print(\"image file read...\")\r\n",
    "  img = process_img(img)\r\n",
    "  mask = process_img(mask)\r\n",
    "  #mask = tf.image.grayscale_to_rgb(mask)\r\n",
    "  if tf.random.uniform(()) > 0.5:\r\n",
    "    img = tf.image.flip_left_right(img)\r\n",
    "    mask = tf.image.flip_left_right(mask)\r\n",
    "\r\n",
    "  weights = tf.constant([6.0, 1.0, 1.0])\r\n",
    "\r\n",
    "  sample_weights = tf.gather(weights, indices=tf.cast(mask, tf.int32))\r\n",
    "\r\n",
    "  return img, mask, sample_weights\r\n",
    "\r\n",
    "def combine_val_images(file_path,file_path_mask):\r\n",
    "  print(\"combine_images_labels acivated...\")\r\n",
    "  img = tf.io.read_file(file_path)\r\n",
    "  mask = tf.io.read_file(file_path_mask)\r\n",
    "  print(\"image file read...\")\r\n",
    "  img = process_img(img)\r\n",
    "  mask = process_img(mask)\r\n",
    "  #mask = tf.image.grayscale_to_rgb(mask)\r\n",
    "  return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"map\" function will now map the filenames into the corresponding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=training_dataset.map(combine_train_images,num_parallel_calls=tf.data.AUTOTUNE)\r\n",
    "val_dataset = validation_dataset.map(combine_val_images,num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = train_dataset.cache().shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\r\n",
    "training_dataset = training_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    "\r\n",
    "validation_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to define the model needed for image segmentation (U-net). Here I'm using MobileNetV2 as the decoder part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[224, 224, 3], include_top=False)\r\n",
    "\r\n",
    "# Use the activations of these layers\r\n",
    "layer_names = [\r\n",
    "    'block_1_expand_relu',   \r\n",
    "    'block_3_expand_relu',  \r\n",
    "    'block_6_expand_relu',  \r\n",
    "    'block_13_expand_relu',  \r\n",
    "    'block_16_project',      #8x8\r\n",
    "]\r\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\r\n",
    "\r\n",
    "# Create the feature extraction model\r\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\r\n",
    "\r\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define the upsample block which consists of multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\r\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\r\n",
    "\r\n",
    "  result = tf.keras.Sequential()\r\n",
    "  result.add(\r\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\r\n",
    "                                    padding='same',\r\n",
    "                                    kernel_initializer=initializer,\r\n",
    "                                    use_bias=False))\r\n",
    "\r\n",
    "  result.add(tf.keras.layers.BatchNormalization())\r\n",
    "\r\n",
    "  if apply_dropout:\r\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\r\n",
    "\r\n",
    "  result.add(tf.keras.layers.ReLU())\r\n",
    "\r\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stack = [\r\n",
    "    upsample(512, 3),  \r\n",
    "    upsample(256, 3),  \r\n",
    "    upsample(128, 3),  \r\n",
    "    upsample(64, 3),   #112x112\r\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all the componenets defined above into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(output_channels):\r\n",
    "  inputs = tf.keras.layers.Input(shape=[224, 224, 3])\r\n",
    "\r\n",
    "  # Downsampling through the model\r\n",
    "  skips = down_stack(inputs)\r\n",
    "  x = skips[-1]\r\n",
    "  skips = reversed(skips[:-1])\r\n",
    "\r\n",
    "  # Upsampling and establishing the skip connections\r\n",
    "  for up, skip in zip(up_stack, skips):\r\n",
    "    x = up(x)\r\n",
    "    concat = tf.keras.layers.Concatenate()\r\n",
    "    x = concat([x, skip])\r\n",
    "\r\n",
    "  # This is the last layer of the model\r\n",
    "  last = tf.keras.layers.Conv2DTranspose(\r\n",
    "      output_channels, 3, strides=2,\r\n",
    "      padding='same')  #224x224\r\n",
    "\r\n",
    "  x = last(x)\r\n",
    "\r\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing and Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(OUTPUT_CHANNELS)\r\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\r\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model to see how it looks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining helper functions to manipulate and display images adn show predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\r\n",
    "  plt.figure(figsize=(15, 15))\r\n",
    "\r\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\r\n",
    "\r\n",
    "  for i in range(len(display_list)):\r\n",
    "    plt.subplot(1, len(display_list), i+1)\r\n",
    "    plt.title(title[i])\r\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\r\n",
    "    plt.axis('off')\r\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in val_dataset.take(5):\r\n",
    "  sample_image, sample_mask = image, mask\r\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\r\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\r\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\r\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=10):\r\n",
    "  if dataset:\r\n",
    "    for image, mask in dataset.take(num):\r\n",
    "      pred_mask = model.predict(image)\r\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\r\n",
    "  else:\r\n",
    "    display([sample_image, sample_mask,\r\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic callback class to visualize the predictions after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\r\n",
    "  def on_epoch_end(self, epoch, logs=None):\r\n",
    "    clear_output(wait=True)\r\n",
    "    show_predictions()\r\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(training_dataset, epochs=EPOCHS,\r\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\r\n",
    "                          validation_steps=VALIDATION_STEPS,\r\n",
    "                          validation_data=validation_dataset,\r\n",
    "                          callbacks=[DisplayCallback()]\r\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\r\n",
    "val_loss = model_history.history['val_loss']\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\r\n",
    "plt.plot(model_history.epoch, val_loss, 'b', label='Validation loss')\r\n",
    "plt.title('Training and Validation Loss')\r\n",
    "plt.xlabel('Epoch')\r\n",
    "plt.ylabel('Loss Value')\r\n",
    "plt.ylim([0, 1])\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the model's accuracy on validation dataset visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally edit a video to hide all the TVs and Laptops in the video (you can have your own video as the input and see the results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('vid1.mp4')\r\n",
    "\r\n",
    "while(cap.isOpened()):\r\n",
    "    ret, frame = cap.read()\r\n",
    "    cv2.imshow('frame', frame)\r\n",
    "    frame = cv2.resize(frame,(224,224))\r\n",
    "    img = tf.image.convert_image_dtype(frame[tf.newaxis, ...], tf.float32) \r\n",
    "    img = create_mask(model.predict(img))\r\n",
    "    img = tf.keras.preprocessing.image.array_to_img(img)\r\n",
    "    img = np.array(img.getchannel(0))\r\n",
    "    img = cv2.GaussianBlur(img, (7, 7), 3)\r\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15,15))\r\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\r\n",
    "    rect, img = cv2.threshold(img, 1, 255, cv2.THRESH_BINARY)\r\n",
    "\r\n",
    "    background = cv2.bitwise_not(img)\r\n",
    "    background = cv2.bitwise_and(frame, frame, mask=background)\r\n",
    "\r\n",
    "    masked_img = cv2.bitwise_and(frame, frame, mask=img)\r\n",
    "    masked_img = cv2.GaussianBlur(masked_img, (27, 27), 2)\r\n",
    "    img = cv2.bitwise_or(masked_img, background)\r\n",
    "    img = cv2.resize(img,(352,640))\r\n",
    "    cv2.imshow('imgage',img)\r\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n",
    "        break\r\n",
    "\r\n",
    "cap.release()\r\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b480f749ce4ef3b2bb2dc91bb87ab66f696b937cd35ce6ca033d1d9ef37b676"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}